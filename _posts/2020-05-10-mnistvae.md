---
layout: post
title: 'MNIST Variational Autoencoder & t-SNE Visualisation'
date: 2020-05-10
comments: true
tags: [pytorch, mnist, VAE]
---

# Introduction

Recently, I have used Variational Autoencoders as part of my PhD studies.  I had some understanding of what VAEs are but I wasn't quite aware of the inner workings.  I like to understand how things work so I wanted to implement it from scratch.  This is a post showing the implementation of a VAE using convolutional layers to learn directly on images.  I couldn't find another implementation online for PyTorch at the time so this was a good opportunity.

This post acts as log to be refered back to, so feel free to comment.  This post also assumes some basic knowledge of neural networks and Bayesian methods.

# Variational Autoencoders

## The Theory
We have:
* dataset **X**
* latent variables $$z$$
* a distribution over datapoints $$P(X)$$

We want to generate new samples that look similar to every $$x\in X$$.  This can be performed with generative models where we want to find a vector of $$z$$ latent random values.  We can use this vector to sample from a probability density function $$P(z)$$ and pass it through some function $$f(z;θ)$$.  The result of this function is a sample similar to datapoints in **X**.  $$f(z;θ)$$ is also a random variable and thus is a probability distribution of $$P(X\|z;θ)$$.  $$\theta$$ correspond to neural network parameters here.

We now want to sample from $$P(X\|z;θ)$$ such that the probability of the sample $$P(X)$$ is high.  This is called maximum likelihood in the equation below.

\begin{equation}
    P(X) = \int P(X\|z;\theta)P(z)dz
\end{equation}

To sample from $$P(z)$$ however we need to sample from a distribution $$P(z\|X)$$ that will likely produce datapoints in **X**. This is possible according to variational Bayesian methods. We learn a probability distribution $$Q(z\|X)$$ close to $$P(z\|X)$$ in order to infer $$P(z)$$.

Using the KL divergence (D), which measures how different two probability distributions are, we define the following:

\begin{equation}
\label{eq:caushy-shwarz}
    D[Q(z)||P(z|X)] = E_{z\sim Q}[\log Q(z|X) - \log P(z|X)]
\end{equation}

Using Bayes rule we can define:

\begin{equation}
P(z|X) = \frac{P(X|z)P(z)}{P(X)}
\end{equation}

and replace the above equation into (2) where after some rearrangements becomes:

\begin{equation}
    D[Q(z|X)||P(z|X)] = E_{z\sim Q}[\log Q(z|X) - \log P(X|z) -\log P(z)] + \log P(X)
\end{equation}

\begin{equation}
    \log P(X) - D[Q(z|X)||P(z|X)] = E_{z\sim Q}[\log P(X|z)] - D[Q(z|X)||P(z)]
\end{equation}

Equation (5) constitutes the VAE objective function.

# Implementation

To implement the objective function we need to make a choice of what form $$Q(z\|X)$$ can take.  We can use a Gaussian distribution for $$Q(z\|X)$$ and $$P(z)$$ resulting in the KL divergence of two Gaussian distributions for the right-most term in (5).  In [1] this is implemented as:

\begin{equation}
    D = \frac{1}{2} * \sum (1 + log(\sigma^2) - \mu^2 - \sigma^2)
\end{equation}

Supplied with the encoder part, $$Q(z\|X)$$, and the decoder $$P(X\|z)$$ we can now train our model.  However, when sampling from $$Q(z\|X)$$ to get the latent variables we are introducing a stochastic unit in our neural network.  This introduces a non-continuous operation and we can't back propagate the error.  To do this, we use something called the "reparameterization trick" identified by [1].  Given the mean and covariance of the encoder we can sample from such a distribution that is differentiable in the following way:

\begin{equation}
    \epsilon \sim \mathcal{N}(0, I)\\\
    z = \mu (X) + \Sigma^{1/2}(X) * \epsilon
\end{equation}

# Where is the Code?
For the neural network implementation I wanted to implement a VAE that learned directly from an image using several convolutional layers before finally flattening the feature vector to be passed through fully connected layers.  Below the VAE along with the Encoder and Decoder are shown.

{% highlight python linenos %}
class Encoder(nn.Module):

    def __init__(self, z_dim, fc1_size):
        super(Encoder, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, 3, 1, 1)
        self.bn_conv1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 32, 3, 1, 1)
        self.bn_conv2 = nn.BatchNorm2d(32)
        
        self.fc1 = nn.Linear(fc1_size, 400)
        self.bn_fc1 = nn.BatchNorm1d(400)
        self.z_mu = nn.Linear(400, z_dim)
        self.z_scale = nn.Linear(400, z_dim)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.bn_conv1(x)
        x = F.max_pool2d(x, 2)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.bn_conv2(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.bn_fc1(x)
        z_loc = self.z_mu(x)
        z_logvar = self.z_scale(x)

        return z_loc, z_logvar

class Decoder(nn.Module):

    def __init__(self, z_dim, output_size):
        super(Decoder, self).__init__()

        self.fc1 = nn.Linear(z_dim, 400)
        self.bn_fc1 = nn.BatchNorm1d(400)
        self.fc2 = nn.Linear(400, output_size)
        self.bn_fc2 = nn.BatchNorm1d(output_size)

        self.conv1 = nn.Conv2d(32, 64, 3, 1, 1)
        self.bn_conv1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 1, 3, 1, 1)
    
    def forward(self, z_input):
        x = self.fc1(z_input)
        x = F.relu(x)
        x = self.bn_fc1(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.bn_fc2(x)
        
        x = x.view(z_input.size()[0], 32, 7, 7)
        x = F.interpolate(x, (14, 14))
        x = self.conv1(x)
        x = F.relu(x)
        x = self.bn_conv1(x)
        x = F.interpolate(x, (28, 28))
        x = self.conv2(x)
        output = torch.sigmoid(x)

        return output

class VAE(nn.Module):

    def __init__(self, z_dim):
        super(VAE, self).__init__()
        self.encoder = Encoder(z_dim, 1568)
        self.decoder = Decoder(z_dim, 1568)

    def forward(self, x):

        z_mean, z_logvar = self.encoder(x)
        std = torch.exp(0.5*z_logvar)
        eps = torch.randn_like(std)
        output = self.decoder(z_mean+eps*std)

        return output, z_mean, z_logvar

    def reconstruct_digit(self, sample):
        return self.decoder(sample)
{% endhighlight %}

The original loss from [1] uses the standard deviation.  However in this implementation I treat *z_scale* layer as the log variance.

Given that we need to reconstruct the input we pass through our network, we also need to define a reconstruction loss:

{% highlight python linenos %}
def loss_fn(output, target):
    bce = F.binary_cross_entropy(output[0], target, reduction='sum')
    
    kl = -0.5 * torch.sum(1 + output[2] - output[1].pow(2) - output[2].exp())

    return bce + kl
{% endhighlight %}

# Visualisation
After training the VAE I wanted to visualise the latent space.  To do this I iterated over the validation set and stored the images and targets.  This was implemented with t-SNE using 2 components.  The code and the result are shown below:

{% highlight python linenos %}
latent = np.array(latent_mnist)
target = np.array(target)
tsne = TSNE(n_components=2, init="pca", random_state=0)

X = tsne.fit_transform(latent)

data = np.vstack((X.T, target)).T
df = pd.DataFrame(data=data, columns=["z1", "z2", "label"])
df["label"] = df["label"].astype(str)

fig = px.scatter(df, x="z1", y="z2", color="label")

pio.write_html(fig, file="raw.html", auto_open=True)
{% endhighlight %}

<iframe id="igraph" scrolling="no" style="border:none; " seamless="seamless" src="{{ site.baseurl }}/assets/html/mnist.html" height="525" width="100%"></iframe>

Thank you for reading! You can find the whole code [here](https://github.com/npitsillos/mnist-vae) and please feel free to leave any comments!

# References
[1] "Auto-Encoding Variational Bayes" [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)

[2] Tutorial on Variational Autoencoders [https://arxiv.org/pdf/1606.05908.pdf](https://arxiv.org/pdf/1606.05908.pdf)